# U-Net with Temporal Attention Encoder (UTAE) for Cell Segmentation

## The project:
This project consists in an implementation of the U-Net with Temporal Attention Encoder (UTAE) model to evaluate the efficiency of temporal attention for budding yeast cells segmentation. More specifically, the task is to predict a mask that segment cells given a lab microscopic image, the model therefore being used for binary classification in our case.

Images were provided by [Laboratory of the Physics of Biological Systems](https://www.epfl.ch/labs/lpbs/), with which we collaborated for the project.

For extensive information about the project motivation, implementation and results, see the project report available in `project_report.pdf`

## Datasets:

 ### ðŸ“‚ `dtsub/train_input` and ðŸ“‚ `dtsub/train_mask` - Training set of 807 images:
 - Image size is 256p x 256p
 - Input images are RGB
 - Mask images are in binary. **0 if the pixel belong to background and 1 if the pixel belong to a budding yeast cell.**


 ### ðŸ“‚ `dtsub/val_input` and ðŸ“‚ `dtsub/val_mask` - Validation set of 121 images:
 - Everything as above, except the transformation performed on the images.


## Architecture
**All files labeled with an asterisk contain code directly taken from the "Panoptic Segmentation Of Satellite Image Time Series With Convolutional Temporal Attention Network" research paper.**

- ðŸ“‚ `image_preprocessing`: 
    -  `split_data_crop_set_3.py` & `data_augmentation.py` : Used to generate the data used to train the model.
- ðŸ“‚ `model` :  Contains the python files used for the construction of UTAE model, the dataset's creation and the metrics definition & calculation.
    - ðŸ“œ `dataset.py`: Loads the dataset properly.
    - ðŸ“œ `metrics.py *`: Computes the confusion matrix and mIoU (code from the article "Panoptic Segmentation Of Satellite Image Time Series With Convolutional Temporal    Attention Network")
    - ðŸ“œ `mIoU.py *`: calculate IoU accuracy (code from the article "Panoptic Segmentation Of Satellite Image Time Series With Convolutional Temporal Attention Network")
    - ðŸ“œ `model.py *`: implementation of UTAE model (code from the article "Panoptic Segmentation Of Satellite Image Time Series With Convolutional Temporal Attention Network")
    - ðŸ“œ `mltae.py *`: implementation of Lightweight Temporal Attention Encoder (L-TAE) for image time series and Multi-Head Attention module (code from the article "Panoptic Segmentation Of Satellite Image Time Series With Convolutional Temporal Attention Network")
    - ðŸ“œ `positional_encoding.py *`: implementation of positional encoder (code from the article "Panoptic Segmentation Of Satellite Image Time Series With Convolutional Temporal Attention Network")
    - ðŸ“œ `weight_init.py *`: Initializes a model's parameters (code from the article "Panoptic Segmentation Of Satellite Image Time Series With Convolutional Temporal Attention Network")
    - ðŸ“œ `utils.py`: load the training and test set and save the predicted images.

- ðŸ“œ `train_params.py`: Hyperparameters tuning of our UTAE model, storing the results in ðŸ“‚ `result_augmented`

- ðŸ“œ `train_argparse.py`: tune a train UTAE model and save predicted image on `saved_images` folder. You can modify the hyperparameter and the folder via the terminal.

- ðŸ“‚ `plot_graph_result`: 
    -  ðŸ“œ `plot_results.ipynb` : Plots the results (using the training from ðŸ“œ `train_params.py`). We put on the repository on `result` the result that we get for different training.


## Prerequisites
Latest versions of :
- pyTorch
- Tensorflow
- Tqdm
- Albumentations

## Run Instructions 

Data is stored in tiff file in ðŸ“‚ `cropped`. To get access to our final dataset, you have to run image_preprocessing/split_data_crop_set_3.py, which extract the set of images and split them on train and test set and store them in "dtsub" folder, and image_preprocessing/data_augmentation.py, which creates more data on the training set.

Our model is based on the grouping of time-series images. To reproduce our results, run the following command: 

    python train_argparse.py --epochs 100 --groupby 7 --maskpos 6

Adapt the command accordingly for the epochs, groupby, and maskpos arguments of your linking.

## Results 

In our case, we used `train_params.py` to find the best groupby value and mask position to predict. We used this on augmented data, and found that `groupby`=7 and `maskpos`=6 give us the best result, evaluted on 100 epochs. The other hyperparameters used are indicated as default ones in the `train_argparse.py` file. The maskpos is highly suceptible to stochastic noise, so it is not an absolute result.

## Reference Paper:

The U-TAE model and the code was adapted from the following paper :

Vivien Sainte Fare Garnot and Loic Landrieu, "[Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks](https://arxiv.org/abs/2104.05495)," ICCV, 2021.


## Team:

  - Yassine Jamaa : yassine.jamaa@epfl.ch
  - Virginie Garnier : virginie.garnier@epfl.ch 
  - Romain Rochepeau : romain.rochepeau@epfl.ch